# | Generative AI: Prompt Engineering Basics |
# → PROMPT ENGINEERING: TECHNIQUES & APPROACHES

In this module, you will discover techniques for skillfully crafting prompts that effectively steer generative AI models. You will also learn about various prompt engineering approaches that can enhance the capabilities of generative AI models to produce precise and relevant responses.

Learning Objectives:

    Apply prompt engineering techniques for writing effective text prompts.
    Apply prompt engineering approaches to optimize the response of generative AI models.


# TEXT-TO-TEXT Prompt Techniques

Welcome to text-to-text prompt techniques.
After watching this video, you'll be able to describe the techniques
using which text prompts can improve the reliability of large language models.
You'll also be able to explain the benefits of using text prompts with large
language models effectively.
In recent years, there has been a significant advancement in natural
language processing, or NLP, by using large language models, or LLMs.
However, as the size and complexity of LLMs continue to increase, questions
about their reliability, security, and potential biases have surfaced.
Using text prompts effectively is a promising solution to these concerns.
Text prompts are carefully crafted instructions that direct LLM behavior to
generate a desired output.
However, the quality and relevance of the generated output depend on
the effectiveness of the prompt and the capability of the LLM.
Let's explore the techniques that make text prompts effective and
improve the reliability of output generated by LLMs.
Let's begin by exploring the task specification technique.
Text prompts should explicitly specify the objective to the LLM to
increase accurate responses.
For example, the prompt translate this English sentence into French,
is a clear directive for achieving the task.
Contextual guidance is a technique using which text prompts provide
specific instructions to the LLMs to generate relevant output.
For example, if you'd like the model to generate a short write up on the landmarks
of New York City, a prompt like write a short paragraph on New York City would
yield a generic response that might not cover what you want.
On the other hand, a more specific prompt like write a short paragraph on
New York City, highlighting its iconic landmarks, would generate a more
appropriate output because of the context included in the prompt.
Domain expertise is also essential to improving LLM dependability.
Text prompts can use domain specific terminology when you need LLMs to generate
content in specialized fields, like medicine, law, or engineering,
where accuracy and precision are crucial.
Let's say you'd like to request medical information regarding hypothyroidism.
Your prompt could read something like, please explain the causes, symptoms, and
treatments of hypothyroidism,
including the latest research and medical guidelines.
Bias mitigation is a technique in which text prompts provide explicit instructions
to generate neutral responses.
For instance, let's assume that you are concerned about a gender bias in
the model's response to a prompt for a writeup on leadership qualities.
To address this, you can use a text prompt like this,
write a 100-word paragraph on leadership traits without favoring any gender.
Provide equal examples of traits from all genders.
Framing is yet another technique by which text prompts guide LLMs to generate
responses within required boundaries.
Suppose you'd like the model to summarize a lengthy article about climate change.
Your text prompt can be, provide a summary of 100 words of the article on climate
change, focusing on its primary findings and recommendations.
Did you know that LLMs today trained on large amounts of data and
tuned to follow instructions, can perform tasks zero-shot.
Zero-shot prompting is a method using which generative AI models generate
meaningful responses to prompts without needing prior training on these specific
prompts.
For example, the prompt could be, select the adjective in this sentence.
The sentence is Anita bakes the best cakes in the neighborhood.
The output here would be best.
However, often you will not get the desired response in one prompt, and
you may need to iterate.
This is where the user feedback loop technique comes in,
wherein users provide feedback to text prompts and
iteratively refine them based on the response generated by the LLM.
This loop allows users to improve the model's output quality incrementally,
until the desired state is achieved.
For example, the user asks the model to write a poem via a text prompt,
the LLM generates a poem.
The user says, make it more humorous.
The LLM adjusts the poem to include more humorous elements.
The user approves the revised poem.
Similarly, for complex tasks, when you are unable to describe your needs clearly,
a technique called a few-shot is used.
It enables in context learning wherein demonstrations are provided in the prompt
to steer the model to better performance.
The demonstrations act as conditioning for
subsequent examples where you would like the model to generate a response.
For instance, suppose the task for
the model is to generate short travel recommendations.
As few-shot prompts, you provide the following guided context to the model.
Recommend a summer travel destination well known for beautiful beaches.
Suggest a fall travel destination that is renowned for its beautiful foliage.
After using these few-shot prompts, the model can generate travel recommendations
for other types of vacations.
For example, if the task is recommend a city to explore.
The model will generate an answer, consider visiting a vibrant city like
Paris, known for its rich history, art, and iconic landmarks.
This is how the model can generate travel recommendations for different types of
vacations, based on the minimal training data provided in the few-shot prompts.
There are several benefits when you use text prompts with LLMs,
using the methods we just discussed.
Let's look at some of them.
The LLM's explainability is enhanced.
Explainability refers to the degree to which a user can understand and interpret
the model's decision making process, and the reasons behind its generated outputs.
Explainability helps users, developers, and
stakeholders understand how the model works.
Why it makes certain predictions or generates specific text.
And whether it can be trusted in various applications.
Explainability is crucial to addressing ethical concerns related to AI.
It helps all stakeholders evaluate and ensure the LLM's behavior is
consistent with the specific domain's ethical guidelines and legal requirements.
In addition to increasing the reliability and explainability of LLMs,
effective text prompts also foster trust between the user and the LLM.
When the user can understand how the LLM works and
see the direct influence of their instructions on the LLM's behavior,
it leads to transparent and meaningful interactions between the user and the LLM.
In this video, you learned the various techniques using which text prompts can
improve the reliability and quality of LLMs.
Specifically, you looked at task specification, contextual guidance,
domain expertise, bias, mitigation framing, and the user feedback loop.
You also learned about the zero-shot and few-shot techniques.
Finally, you learned the several benefits of using text prompts with LLMs
effectively, such as increasing the explainability of LLMs,
addressing ethical considerations, and building user trust.



# TEXT-TO-IMAGE Prompt Techniques

Welcome to text-to-image prompt techniques.
After watching this video,
you'll be able to explain
common image prompting techniques
used to improve the quality and impact of images,
and apply these techniques to write
better prompts for image generation.
Images are an essential part of communication
and are used in various fields such as marketing,
advertising, education, journalism, and many others.
Nonetheless, certain images excel in
their ability to convey emotions
more effectively than others.
An image prompt is a text description
of an image that you want to generate.
It can be as simple as a single word or phrase,
or it can be more detailed describing the composition,
colors, and mood of the image.
To increase the impact of images obtained through
generative AI models and
make them more convincing and compelling,
you can use image prompting techniques.
These techniques aim to improve the quality, diversity,
and relevance of images produced by generative AI models.
There are different image prompting techniques that
can be used to improve the impact of images.
Let's learn about these techniques one by one.
Style modifiers are descriptors used to influence
the artistic style or
visual attributes of images
produced by generative AI models.
These descriptors can help
the model produce graphics with
innovative style while conforming to
the structure and content of the input prompt.
You can modify the various visual elements
of an image like color, contrast,
texture, shape, and size,
and generate output that is
aesthetically appealing and visually pleasing.
Your prompt can include information about
miscellaneous art styles, historical art periods,
photography techniques, types of art materials used,
and even traits of well-known brands
or artists you want the model to emulate.
All this information can help the generative model
understand the desired appearance
or style of the output image.
Here are a few examples of
style modifiers used in image prompts.
The style modifiers used
in these prompts have been highlighted.
Moving on to the next image prompting technique,
that is quality boosters.
High-quality images are more
convincing and reliable as compared to low-quality ones.
Images with low resolution
frequently exhibit blurriness and pixelation,
making it difficult for viewers to
discern the finer details within the image.
On the other hand, images with high-resolution
guarantee essential visibility and readability.
The perceived worth of an image can be
raised by using high-quality graphic design.
Quality boosters are terms
used in an image prompt to enhance
the visual appeal and improve
the overall fidelity and sharpness of the output.
These are specific terms that can direct
the generative AI model to
perform steps like noise reduction,
sharpening, color correction, and resolution enhancement.
You can use terms like high resolution,
2k, 4k, hyper-detailed,
sharp focus, complimentary colors,
and many others in your image prompts
as quality boosters.
They can enhance specific features of the image,
resulting in more coherent output.
Let's look at some examples to understand how
quality boosters can be used in image prompts.
Terms such as highlights the texture,
4k resolution, sharp, crisp details,
and fine lines, complementary colors,
blurred background, and stand out
are quality boosters used in the given image prompts.
The third image prompting technique is repetition.
This technique leverages the power of iterative sampling
to enhance the diversity of
images generated by the model.
Repetition involves emphasizing
a particular visual element
within an image to create
a sense of familiarity for the model,
allowing it to focus on
a specific idea or concept you want to highlight.
This can be accomplished by repeating
the same word or similar phrase within the image prompt.
Repetition helps reinforce the message conveyed
through the image and increase
the memorability of the model.
Rather than producing just one image based on a prompt,
the model generates multiple
images with subtle differences,
resulting in a diverse set of potential outputs.
This technique is particularly
valuable when generative models are confronted with
abstract or ambiguous prompts to
which numerous valid interpretations are possible.
Let's look at some examples of
repetitive words used in an image prompt.
Words such as tiny,
dense, enormous, vast, serene,
clear, and lush have been repeated
many times to focus on a specific idea.
The fourth image prompting technique is weighted terms.
Weighted terms refer to the use of words or phrases that
can have a powerful emotional or psychological impact.
For example, words such as free,
limited time offer, and guaranteed,
are often used in advertising to
elicit a sense of urgency, security, and trust.
Similarly, words such as luxury, premium,
and exclusive are used to create
a sense of exclusivity and sophistication.
Generative AI models allow you to
give positive or negative weights to such terms,
to emphasize or de-emphasize a certain emotion.
Using weighted terms in an image prompt can
help create images that are memorable and convincing,
and can draw emotional responses from the audience.
Here are some examples of
weighted terms used in an image prompt.
As you can see in the first example,
a weight of positive ten is given to the word warm,
whereas the weight of crackling is positive eight.
This means the generative model must focus more
on the word warm and a little less on the word crackling.
Similarly, in the second example,
a positive six weight is given to the word shimmering,
and the weight of neon-lit is positive eight,
so the model should focus more on neon-lit.
Whereas in the last example,
negative weight of six is given to the word colorful,
and a positive weight of ten is given to exotic.
This means the model must emphasize the word
exotic and de-emphasize the word colorful.
The fifth image prompting technique
is fix deformed generations.
This technique is used to modify any deformities
or anomalies that may impact
the effectiveness of the image.
Deformities in an image can
include conditions like distortion,
particularly on human body parts like hands or feet,
pixelation or other image quality issues that
can detract from the visual appeal
and clarity of the image.
This can be mitigated to
some extent by using good negative prompts.
Here are some examples of
deformed generation prompting
techniques used in image prompts.
You can see that in all these examples,
good negative words have been used to
mitigate the issues of deformed images.
In this video, you learned that
image prompting techniques play a vital role
in advancing the image generation capabilities
of generative AI models.
Style modifiers, quality boosters,
repetition, weighted terms,
and fix deformed generations are
five techniques that can be used to
improve the impact of images produced.
By incorporating these techniques,
one can create more memorable, engaging,
and persuasive visuals that can
effectively communicate the intended message. 


## Hands-on Lab: Effective Text Prompts for Image Generation



# TYPES OF APPROACHS

## Interview Pattern Approach

Welcome to interview pattern approach, after watching this video,
you'll be able to explain the interview pattern approach to prompt engineering.
You'll also learn to apply this concept to write more effective prompts for
generative AI models to produce more specific responses.
The interview pattern approach to prompt engineering is a strategy that involves
designing prompts by simulating a conversation or
interacting with the model in the style of an interview.
Let's learn how the interview pattern approach typically works.
This approach requires meticulous optimization of the prompt to
ensure the model generates responses that precisely meet your objectives.
It involves providing specific prompt instructions to the model in response to
which the model asks necessary follow-up questions from the user.
Depending on the response to these follow-up questions,
the model draws information relevant to its task, processes it, and
provides a well optimized response to the user.
The more information you provide, the better the result will be.
You'll understand this better with the help of an example.
Suppose you want the model to behave like a travel consultant and
plan your travel itinerary for a vacation.
How will you prompt the model to do so?
You can give prompt instructions to the model saying you will act as a seasoned
travel expert.
Your objective is to engage in a comprehensive trip planning session
with me.
Begin by asking a series of detailed questions, one at a time.
To gather all the essential information required to craft the most tailored and
memorable travel itinerary based on my specific preferences, interests,
and budget.
In response to this prompt instruction,
the model will ask all the required follow-up questions,
such as what types of destinations do you enjoy traveling to the most?
Could you describe your ideal vacation in terms of activities and experiences?
How do you typically plan your trips,
and what factors are most important to you when choosing a destination?
Do you find any specific cultural or
historical aspects intriguing when planning your travel destination?
What kind of accommodation options do you prefer when you travel, and why?
How do you balance budget considerations with the desire for
a memorable travel experience?
In this example, each question builds upon the previous one,
creating a structured and informative conversation about travel preferences.
Depending on your response to these queries, the model will plan a memorable
travel itinerary that aligns with your preferences and needs.
In this video, you learned that the interview pattern approach is superior to
the conventional prompting approach.
As it allows a more dynamic and
iterative conversation when interacting with generative AI models.
Instead of providing a single static prompt, the interview pattern involves
a back-and-forth exchange of information with the model,
which helps clarify queries and guides the response of the model in real time.
This, in turn enhances the capabilities of the users to
optimize the results obtained.


## Hands-on Lab: The Interview Pattern Approach in Prompt Engineering


## Chain-of-Thought Approach

Welcome to Chain-of-Thought Approach.
After watching this video,
you'll be able to explain
the chain-of-thought approach to prompt engineering.
You'll also learn to apply this concept to
develop prompts with compelling and diverse examples,
allowing generative AI models to understand
the context and generate
coherent responses with ongoing conversation.
Chain-of-thought is a prompt-based learning approach
that involves constructing
a series of prompts or questions to
guide the model to generate a desired response.
Using this approach, you can
demonstrate the cognitive abilities of
the generative AI models and
better explain their reasoning process.
It involves breaking down a complex task into
smaller and easier ones through
a sequence of more straightforward prompts,
with each prompt building upon
the previous one to guide
the models toward the intended outcome.
Before posing a question directly to the model,
you feed it with related questions
along with their corresponding solutions.
This chain-of-prompts
helps the model think about the problem
and use the same strategy to
answer more such questions correctly.
In simpler words, the prompt includes
a question and an accurate answer to
the question to provide the required context
and step-by-step reasoning for the model,
then it poses a different question to
be answered using the same line of reasoning.
You'll understand this better
with the help of an example.
If you pose a mathematical problem to the model like,
Matthew has six eggs.
He buys two more trays of eggs.
Each tray has 12 eggs.
How many eggs does he have now?
This logic can go off the rails,
especially for a complex problem.
To train the model with the appropriate reasoning
involved in solving such questions,
you can pose a similar question like this one.
Mary has eight radishes.
She used five radishes to prepare the dinner.
The next morning she bought 10 more radishes.
How many radishes does she have now?
Along with the questions,
you must provide the correct logical solution as well.
Mary had eight radishes.
She cooked dinner using five of them,
so she had 8-5=3 radishes left with her.
The next morning, she bought 10 more,
so she has 3+10=13 radishes now.
This will help the model understand
the logic involved and come up with the right solution.
Your final prompt should include the following:
A related question with the appropriate solution,
and then another question that can be
solved using the same logic or reasoning.
In this prompt, you can see there's a question,
a logical solution to the question,
and another question that can
be solved using the same logic.
In this video, you learned that
the chain of thought approach strengthens
the cognitive abilities for
generative AI models and
solicits a step-by-step thinking process.
This approach involves providing the model with
related questions and their corresponding solutions,
to train the model with the logic
behind solving these questions so
that the same logic can be
applied to solve more such questions. 


## Hands-on Lab: The Chain-of-Thought Approach in Prompt Engineering


## Tree-of-Thought Approach

Welcome to tree-of-thought approach.
After watching this video, you'll be able to explain the tree-of-thought approach
to prompt engineering.
You'll also learn to apply this approach to draft prompts for
generating tailored responses.
The tree-of-thought is an innovative technique built to expand the capabilities
of the chain of thought prompting approach.
It enables the generative AI models to demonstrate advanced reasoning
capabilities.
It involves hierarchically structuring a prompt or query akin to a tree structure
to specify the desired line of thinking or reasoning for the model.
This approach is particularly useful when you want to provide explicit instructions
or constraints to the model to ensure it generates the desired output.
This method holds immense potential for unlocking new solutions and
tackling complex problems.
Let's understand how the tree-of-thought approach works.
It involves generating multiple lines of thought resembling
a decision tree to explore different possibilities and ideas.
Unlike traditional linear approaches, this technique allows the model to evaluate and
pursue multiple paths simultaneously.
Each thought or idea branches out,
creating a treelike structure of interconnected thoughts.
The model proceeds by assessing every possible route,
assigning numerical values according to its predictions of outcomes.
And eliminating lesser promising lines of thought,
ultimately pinpointing the most favorable choices.
You will understand this better with the help of an example.
Suppose you want the model to design recruitment and retention strategies for
attracting skilled remote employees for an e-commerce business.
You want the model to employ the tree-of-thought approach to do that.
You can give the following prompt instructions to the model.
Imagine three different experts answering this question.
All experts will write down one step of their thinking and
then share it with the group.
Then all experts will go on to the next step, etcc.
If any expert realizes they're wrong at any point, then they leave.
Along with the prompt instruction, you will also give the original question for
the prompt.
Act as a human resource specialist design a recruitment and
retention strategy for an e-commerce business,
focusing on attracting and retaining skilled remote employees.
Building such prompt instruction will allow the generative AI model to consider
a step-by-step process and think logically.
It will also make it consider intermediate thoughts, building upon them, and
exploring branches that may or may not lead somewhere.
This practice will maximize the use and capabilities of the model,
rendering more useful results.
In this video, you learned that the tree-of-thought approach is an innovative
technique that builds upon the chain-of-thought approach and
involves structuring prompts hierarchically.
Akin to a treelike structure to guide the model's reasoning and
generation of output.
This approach is particularly valuable when explicit instructions or
constraints are necessary for desired outputs.
It enables the model to explore various possibilities and
ideas simultaneously, branching out like a decision Tree.

## Hands-on Lab: The Tree-of-Thought Approach in Prompt Engineering



# LESSON SUMMARY MODULE 2

At this point, you have learned the techniques for skillfully crafting prompts that effectively steer generative AI models. You now know the various prompt engineering approaches that optimize the response of generative AI models.

You explored the techniques, including zero-shot and few-shot prompting, using which text prompts can improve the reliability of large language models (LLMs) and yield greater benefits from their responses. You learned how using different approaches such as interview patterns, Chain-of-Thought, and Tree-of-Thought to write prompts helps generative AI models produce more specific, contextual, and customized responses to the user's needs. You even had the opportunity to experience the application of each of these approaches through hands-on lab experiences. You were privy to what experts from the field had to say about the role of prompt engineering in AI.

Specifically, you learned that:

- The various techniques using which text prompts can improve the reliability and quality of the output generated from LLMs are task specification, contextual guidance, domain expertise, bias mitigation, framing, and the user feedback loop. 

- The zero-shot prompting technique refers to the capability of LLMs to generate meaningful responses to prompts without needing prior training.

- The few-shot prompting technique used with LLMs relies on in-context learning, wherein demonstrations are provided in the prompt to steer the model toward better performance.

- The several benefits of using text prompts with LLMs effectively are increasing the explain ability of LLMs, addressing ethical considerations, and building user trust. 

- The interview pattern approach is superior to the conventional prompting approach as it allows a more dynamic and iterative conversation when interacting with generative AI models.

- The Chain-of-Thought approach strengthens the cognitive abilities of generative AI models and solicits a step-by-step thinking process.

- The Tree-of-Thought approach is an innovative technique that builds upon the Chain-of-Thought approach and involves structuring prompts hierarchically, akin to a tree, to guide the model's reasoning and output generation.
